# -*- coding: utf-8 -*-
"""Syntax Analisis Sentimen Data Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127jSePUXgvbLtJhtokPgbkdvuSVlX0EN
"""

#@title Twitter Auth Token

twitter_auth_token = '713df92d6eef5c8e9144caed634ac10b26f73571' # change this auth token

# Import required Python package
!pip install pandas

# Install Node.js (because tweet-harvest built using Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

# Crawl Data

filename = 'kurikulum_merdeka.csv'
search_keyword = 'kurikulum merdeka belajar since:2024-07-01 until:2024-10-22 lang:id'
limit = 300

!npx -y tweet-harvest@2.6.1 -o "{filename}" -s "{search_keyword}" --tab "LATEST" -l {limit} --token {twitter_auth_token}

import pandas as pd

# Specify the path to your CSV file
file_path = f"tweets-data/{filename}"

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path, delimiter=",")

# Display the DataFrame
display(df)

# Cek jumlah data yang didapatkan

num_tweets = len(df)
print(f"Jumlah tweet dalam dataframe adalah {num_tweets}.")

import pandas as pd

# Pilih hanya kolom yang diinginkan
df_selected = df[['created_at', 'full_text']]

# Simpan sebagai file .xlsx
df_selected.to_excel('selected_data.xlsx', index=False)
print("File 'selected_data.xlsx' berhasil dibuat.")

display(df_selected.head())

# Download file .xlsx
from google.colab import files
files.download('selected_data.xlsx')

"""## IMPORT DATA"""

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

uploaded = files.upload()

df = pd.read_excel("Data.xlsx")

# Display the DataFrame
display(df)

"""## EKSPLORASI DATA"""

# Melihat ukuran data
print("Dimensi data:", df.shape)

# Melihat ringkasan informasi data
df.info()

# Menambahkan kolom panjang teks
df['text_length'] = df['full_text'].apply(len)

# Statistik deskriptif panjang teks
print("Statistik panjang teks:")
print(df['text_length'].describe())

# Visualisasi distribusi panjang teks
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.histplot(df['text_length'], bins=20, kde=True)
plt.title("Distribusi Panjang Teks")
plt.xlabel("Jumlah Karakter")
plt.show()

"""### Analisis Frekuensi Kata (Mengidentifikasi kata-kata yang sering muncul)"""

from collections import Counter
from wordcloud import WordCloud

# Gabungkan semua teks menjadi satu string
all_text = " ".join(df['full_text'].tolist())

# Visualisasi word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords='indonesian').generate(all_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title("Word Cloud untuk Semua Teks")
plt.axis("off")
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

# Isi missing values di kolom 'full_text' dengan string kosong
df['full_text'] = df['full_text'].fillna("")

# Melakukan vectorisasi kata
vectorizer = CountVectorizer(max_features=20)  # Hapus parameter stop_words
X = vectorizer.fit_transform(df['full_text'])

# Membuat DataFrame untuk menampilkan kata-kata teratas
top_words_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
top_words_counts = top_words_df.sum().sort_values(ascending=False)

# Menampilkan kata yang paling sering muncul
print("Kata yang paling sering muncul:")
print(top_words_counts)

"""## PRE PROCESSING DATA

### CASE FOLDING (Mengubah Teks Menjadi Huruf Kecil)
"""

import pandas as pd

# Misalkan df adalah DataFrame Anda
# Melakukan case folding pada kolom 'full_text'
df['case_folding'] = df['full_text'].str.lower()

# Tampilkan DataFrame setelah case folding
display(df[['full_text', 'case_folding']].head(10))

"""### REMOVING SPECIAL CHARACTERS (Menghapus Karakter Spesial)"""

import re
import string
import pandas as pd

# Definisikan fungsi untuk membersihkan teks
def bersihkan_teks(teks):
    if not isinstance(teks, str):  # Memastikan input adalah string
        return ''  # Mengembalikan string kosong jika bukan string

    teks = re.sub(r'@[A-Za-z0-9]+', '', teks)  # remove mention
    teks = re.sub(r'#[A-Za-z0-9]+', '', teks)  # remove hashtag
    teks = re.sub(r'RT[\s]+', '', teks)  # remove retweet
    teks = re.sub(r'https?:\/\/\S+', '', teks)  # remove link
    teks = re.sub(r'[0-9]+', '', teks)  # remove numbers
    teks = re.sub(r'[^A-Za-z ]+', '', teks)  # remove karakter non-alphabet

    teks = teks.replace('\n', ' ')  # remove enter
    teks = teks.strip()  # remove spasi di awal dan akhir teks
    return teks

# Misalkan df_CF adalah DataFrame Anda setelah case folding
# Terapkan fungsi bersihkan_teks ke kolom 'full_text'
df['Cleaning'] = df['case_folding'].apply(bersihkan_teks)

# Tampilkan DataFrame setelah pembersihan teks
display(df[['full_text', 'case_folding','Cleaning']].head(10))

"""# **Normalisasi**"""

norm = {
    'kalo': 'kalau', 'klo': 'kalau', 'gak': 'tidak', 'jd': 'jadi', 'ntar': 'nanti',
    'tp': 'tetapi', 'yg': 'yang', 'dr': 'dari', 'kali': 'mungkin', 'beneran': 'benar', 'gaada': 'tidak ada', 'banget': 'sangat',
    'biar': 'agar', 'bikin': 'membuat', 'mau': 'ingin', 'nggak': 'tidak', 'enggak': 'tidak', 'cuma': 'hanya', 'ortus': 'orang tua',
    'ortu': 'orang tua', 'nyambi': 'sampingan', 'samsek': 'sama sekali', 'bodo amat': 'tidak peduli', 'gitu': 'seperti itu',
    'gabisa': 'tidak bisa', 'sdh': 'sudah', 'kmna': 'kemana', 'skrng': 'sekarang', 'jg': 'juga', 'dlu': 'dulu', 'dlm': 'dalam',
    'lgsg': 'langsung', 'dpt': 'dapat', 'bgt': 'banget', 'bs': 'bisa', 'bkn': 'bukan', 'bny': 'banyak', 'utk': 'untuk', 'bljr': 'belajar',
    'bnyk': 'banyak', 'krg': 'kurang', 'bgs': 'bagus', 'jlk': 'jelek', 'ggl': 'gagal', 'kyk': 'kayak', 'eak': 'iya', 'alesan': 'alasan',
    'org': 'orang', 'orng': 'orang', 'skli': 'sekali', 'mdh': 'mudah', 'gampang': 'mudah', 'mentri': 'menteri', 'gru': 'guru', 'hrs': 'harus',
    'tlg': 'tolong', 'jrg': 'jarang', 'tsb': 'tersebut', 'gue': 'saya', 'aku': 'saya', 'trs': 'terus', 'pinter': 'pintar',
    'regis': 'daftar', 'denger': 'dengar', 'cpt': 'cepat', 'cepet': 'cepat', 'pake': 'pakai', 'pdhl': 'padahal', 'adlh': 'adalah',
    'ttg': 'tentang', 'ente': 'kamu', 'pekok': 'bodoh', 'mosok': 'masa', 'jng': 'jangan', 'mmg': 'memang', 'oon': 'bodoh', 'nga': 'tidak',
    'sy': 'saya', 'krn': 'karena', 'negri': 'negeri', 'tolol': 'bodoh', 'dbkin': 'dibuat', 'matpel': 'mata pelajaran', 'blm': 'belum',
    'ogah': 'tidak', 'sebahahiya': 'bahagia', 'mintol': 'minta tolong', 'klau': 'kalau', 'msh': 'masih', 'pd': 'pada',
    'skrng': 'sekarang', 'kls': 'kelas', 'kurmer': 'kurikulum merdeka', 'sbb': 'sebab', 'dsb': 'dan sebagainya', 'ndak': 'tidak', 'and': 'dan',
    'midah': 'mudah', 'bnget': 'sangat', 'bocil': 'anak kecil','kagak' : 'tidak', 'Top' : 'bagus', 'dg' : 'dengan', 'pakait' : 'paket',
    'ulangan':'ujian', 'jkt':'jakarta', 'bkn' : 'bukan', 'gaada' : 'tidak ada', 'banget' : 'sangat', 'biar' : 'agar', 'bikin' : 'membuat',
    'mau' : 'ingin', 'nggak' : 'tidak', 'enggak' : 'tidak', 'cuma' : 'hanya', 'ortus' : 'orang tua', 'ortu' : 'orang tua', 'nyambi' : 'sampingan',
    'samsek' : 'sama sekali', 'bodo amat' : 'tidak peduli', 'gitu' : 'seperti itu', 'gabisa' : 'tidak bisa', 'sdh' : 'sudah', 'kmna' : 'kemana',
    'skrng' : 'sekarang', 'jg' : 'juga', 'dlu' : 'dulu', 'dlm' : 'dalam', 'lgsg' : 'langsung', 'dpt' : 'dapat', 'bgt' : 'banget', 'bs' : 'bisa', 'ama':'sama', 'scr':'secara',
    'ngajari':'mengajarkan','amp':'sampai', 'indo':'indonesia', 'sbg':'sebagai', 'udah':'sudah'
}

# Fungsi normalisasi
def normalisasi(str_text):
    for i in norm:
        str_text = str_text.replace(i, norm[i])
    return str_text


# Penerapan pada DataFrame
df['Normalisasi'] = df['Cleaning'].apply(lambda x: normalisasi(x))
display(df[['Cleaning', 'Normalisasi']].head(305))

"""# **TOKENISASI**"""

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab')

# Definisikan fungsi tokenisasi
def tokenisasi(text):
    return word_tokenize(text)

# Sekarang Anda bisa memanggil fungsi tokenisasi di dalam apply()
df['Tokenisasi'] = df['Normalisasi'].apply(lambda x: tokenisasi(x))

# Menampilkan hasil
display(df[['Normalisasi','Tokenisasi']].head(10))

"""# **StopWord**"""

from nltk.corpus import stopwords
import nltk

# Pastikan resource stopwords sudah didownload
nltk.download('stopwords')

# Ambil stopword bawaan NLTK untuk bahasa Indonesia
stop_words = set(stopwords.words('indonesian'))

# Tambahkan stopword manual berdasarkan hasil analisis data
manual_stopwords = {'yg', 'dgn', 'juga', 'dll', 'sd', 'smp', 'sma', 'ga','tuh', 'tp', 'tdk', 'gk', 'aja','pas', 'nya', 'nih', 'dong', 'deh','lha', 'nih', 'dong', 'deh', 'tp', 'tdk', 'gk', 'aja', 'ya', 'stdanard', 'tidakjari',
    'sampitidakn', 'ngtidak', 'tatidakn','qs','yo','wur','the','nafsi','awikwok','cok','w','gok','wkwkw', 'ulatidakn','k','q', 'dengann','gek','lu','kek','d','sih','kah','doang', 'trims','lingkutidakn','tik','yk','kek','tekdus','btw', 'lo','masayaarakat','kocak','okay','my','two','cents','on','neh','mumet','waw','wkwkwk'}

# Gabungkan stopword bawaan dengan manual
combined_stopwords = stop_words.union(manual_stopwords)

# Fungsi untuk menghapus stopword
def remove_stopwords_custom(tokens):
    return [word for word in tokens if word.lower() not in combined_stopwords]

# Terapkan fungsi untuk menghapus stopword pada kolom Tokenisasi
df['Stopword'] = df['Tokenisasi'].apply(lambda tokens: remove_stopwords_custom(tokens))

# Menampilkan hasil
display(df[['Tokenisasi', 'Stopword']].head(10))

"""# **LEMMATIZATION**"""

# Import library yang dibutuhkan
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# Download WordNet if not already downloaded
nltk.download('wordnet')

# Normalisasi: Misalnya, menggunakan dictionary 'norm' untuk penggantian
def normalisasi(str_text):
    for i in norm:
        str_text = str_text.replace(i, norm[i])
    return str_text

# Tokenisasi menggunakan NLTK
def tokenisasi(str_text):
    return word_tokenize(str_text)

# Menghapus stopwords menggunakan daftar 'combined_stopwords'
def remove_stopwords_custom(tokens):
    return [word for word in tokens if word.lower() not in combined_stopwords]

# Lemmatization menggunakan NLTK
lemmatizer = WordNetLemmatizer()
def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(word) for word in tokens]

# Lemmatization
df['Lemmatization'] = df['Stopword'].apply(lambda tokens: lemmatize_tokens(tokens))

# Menampilkan hasil
display(df[['Stopword', 'Lemmatization']].head(305))

from collections import Counter

# Gabungkan seluruh kata-kata dari kolom 'Stopword' menjadi satu list
all_words = [word for tokens in df['Lemmatization'] for word in tokens]

# Hitung frekuensi kemunculan kata
word_counts = Counter(all_words)

# Ambil 10 kata yang paling sering muncul
most_common_words = word_counts.most_common(10)

# Menampilkan hasil
print("10 Kata yang Paling Sering Muncul:")
for word, count in most_common_words:
    print(f"{word}: {count}")

"""# **TF IDF**

## Hasilnya digunakan untuk evaluasi model SVMnya
"""

# Daftar kata yang ingin ditampilkan
kata_tertentu = ['merdeka', 'kurikulum', 'belajar', 'sekolah', 'guru', 'pendidikan', 'anak', 'siswa', 'kelas']

# 1. Gabungkan hasil lemmatization menjadi teks kembali
df['Lemmatization_text'] = df['Lemmatization'].apply(lambda x: ' '.join(x))

# 2. Inisialisasi TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(vocabulary=kata_tertentu)  # Hanya menggunakan kata yang ada di list

# 3. Fit dan transform data lemmatization ke dalam representasi TF-IDF
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Lemmatization_text'])

# 4. Mengubah hasil TF-IDF menjadi DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# 5. Menampilkan hasil
display(tfidf_df.head(305))

"""Interpretasi:
Hasil TF-IDF ini membantu mengidentifikasi kata-kata kunci yang relevan dalam masing-masing dokumen, memberikan gambaran yang jelas tentang topik atau tema yang paling dominan dan hasilnya akan digunakan untuk evaluasi model SVMnya

# **LABELING**
"""

# Definisikan kata kunci positif dan negatif yang lebih lengkap
positive_keywords = ['baik', 'efektif','bagus','menyenangkan','baik','pintar','sukses', 'memuaskan', 'terbaik', 'positif', 'berguna',
                     'bermanfaat', 'terampil', 'inovatif', 'unggul', 'kompeten', 'optimis', 'berhasil', 'memotivasi', 'bersemangat', 'terinspirasi',
                     'berdaya', 'terpenuhi', 'produktif', 'solutif', 'berprestasi','lulus', 'terobosan', 'mengagumkan', 'terus maju', 'berhasil', 'cerah',
                     'membangun', 'meriah', 'menguntungkan', 'mendukung', 'penuh semangat','inovasi', 'cemerlang', 'berdaya saing', 'prestasi', 'hebat', 'terbilang', 'diberdayakan',
                     'termotivasi','inspiratif', 'cerdas', 'terpercaya', 'cocok', 'mantap', 'mendalam', 'terhormat', 'terbaik', 'luar biasa', 'berkualitas', 'berarti', 'mulia',
                     'banyak manfaat', 'cerah', 'terpercaya', 'mendalam', 'tidak diragukan', 'memuaskan', 'aktif', 'produktif', 'semangat', 'terbuka', 'memotivasi',
                     'pribadi unggul', 'berpikir positif', 'berintegritas', 'bijaksana', 'berpencapaian', 'bernilai tinggi', 'berani', 'optimistik', 'beradab', 'disiplin',
                     'terstruktur', 'berprestasi tinggi', 'berdaya guna', 'apresiasi','sesuai','manfaat', 'terbaik di kelasnya', 'berani ambil tantangan', 'penuh percaya diri',
                     'semangat pantang menyerah', 'menghargai keberagaman', 'juara', 'bermental juara','cerdas','penuh semangat', 'inspiratif', 'cerdas', 'berpikir maju',
                     'berkomitmen', 'canggih', 'terobosan','sopan','nambah','skill', 'berjiwa pemimpin', 'menginspirasi', 'terobosan baru', 'optimisme', 'terpercaya', 'berkarya', 'pertumbuhan',
                     'berperan aktif', 'positif thinking', 'berdedikasi', 'tercapai', 'bersinar', 'mengagumkan', 'kepercayaan diri', 'berkinerja tinggi', 'mendalam',
                     'penuh rasa syukur', 'terpenuhi', 'berdampak positif', 'berkembangan', 'bekerja sama', 'memajukan', 'penuh motivasi', 'tepat sasaran', 'terpenuhi tujuan',
                     'menguatkan', 'solusi tepat', 'berkualitas tinggi', 'penuh inovasi', 'berdaya cipta', 'mengapresiasi', 'penuh dedikasi', 'berhasil besar', 'berkinerja',
                     'sukses luar biasa', 'meraih sukses', 'berkembang pesat', 'sukses besar', 'keberhasilan', 'berdaya saing tinggi', 'optimalkan', 'membanggakan',
                     'terjaga dengan baik', 'bener-bener','sukses','semangat','fokus','bisa', 'berkembang', 'berkecimpung', 'meningkat', 'menjulang', 'keunggulan', 'hebat',
                     'mendukung penuh', 'bekerja sama', 'terjamin', 'terjamin baik', 'penuh harapan', 'enerjik', 'tangguh', 'sukses besar', 'menjadi juara', 'perkembangan',
                     'peningkatan', 'konsisten', 'terlatih', 'menyenangkan', 'sesuai harapan', 'berorientasi hasil', 'semangat juang', 'berbakat', 'mencapai tujuan', 'semangat membara',
                     'optimis tinggi', 'bermotivasi tinggi', 'berhati besar','meningkat','relevan','setuju']
negative_keywords = ['gagal', 'kesulitan', 'masalah', 'tidak', 'kurang', 'tidak efektif', 'kegagalan','tidak','problem','bukan','malas','ganti',
                     'kesulitan', 'buruk', 'merugikan', 'terhambat', 'keluhan', 'frustrasi', 'terlambat', 'kesalahan', 'tidak jelas', 'stres',
                     'tidak puas', 'menyusahkan', 'merugikan', 'bencana', 'kesalahan', 'kritis', 'kekurangan', 'tertinggal', 'komplikasi', 'buruk',
                     'kecewa', 'tidak jelas', 'negatif', 'mengganggu', 'frustrasi', 'terlambat', 'kesusahan', 'kesulitan', 'takut', 'terpuruk', 'kelam',
                     'kehilangan', 'bahaya', 'ketidakpastian', 'kurang efektif', 'buntu', 'sesat', 'terhenti', 'terisolasi','bodoh','susah','butuh', 'terpuruk',
                     'gagal total', 'tidak mampu', 'keterpurukan', 'bermasalah','dikaji','problematik', 'kalah', 'disesali', 'mengalami kegagalan', 'menyakitkan', 'terhambat kemajuan',
                     'tidak efisien', 'kurang beruntung', 'diserang','tekanan', 'disingkirkan', 'terhalang', 'negatif berpikir', 'terlalu lambat', 'dikhianati', 'tidak stabil',
                     'sakit hati', 'gagal paham', 'lemah', 'terus-menerus terjebak', 'putus asa', 'takut gagal', 'tidak kompeten', 'bermasalah', 'lelah', 'hancur',
                     'terisolasi', 'ditinggalkan', 'stres berat', 'terjebak', 'tersesat', 'terlalu percaya diri', 'kehilangan arah', 'sulit berkembang',
                     'terkesan tidak profesional', 'berhenti berkembang', 'mundur', 'bobrok','tanpa','tidak','terlalu buruk untuk diperbaiki','gacocok','capek', 'tekanan besar', 'tidak bernilai',
                     'berisiko tinggi', 'buntu', 'risiko kegagalan tinggi', 'depresi','berhenti','no', 'frustrasi tinggi', 'kurang terampil', 'batal', 'tidak sesuai harapan', 'terbelakang',
                     'masalah besar', 'terlalu banyak', 'terus-menerus gagal', 'kesulitan tinggi', 'gagal total', 'ga','kehabisan waktu', 'terlalu cepat', 'terbatas',
                     'terlalu lambat', 'buntu jalan', 'keterlambatan', 'putus asa', 'kesulitan berat', 'ketidakpastian tinggi', 'kurang hasil', 'kemunduran', 'terjerumus',
                     'kesalahan fatal', 'terhambat', 'buruk sekali','mengurangi','akibat', 'terasing', 'terlalu berat', 'dikeluhkan', 'tersendat', 'tertinggal jauh', 'terlempar', 'lemah',
                     'tidak pantas','penjilat', 'kehilangan fokus', 'terbelenggu', 'merosot','miris','benci','badut', 'menghalangi', 'frustrasi berat', 'hampir menyerah', 'rugi besar', 'kegagalan serius',
                     'kondisi buruk', 'terancam', 'terisolasi', 'terlalu rumit', 'kekurangan serius', 'kekeliruan', 'kesalahan besar', 'terdampak', 'tidak tepat',
                     'menyusahkan', 'tercekik', 'kerugian besar', 'gagal berfungsi', 'memprihatinkan', 'sangat buruk', 'terpukul', 'tidak produktif', 'berisiko tinggi',
                     'kalah telak', 'terlalu penuh tekanan', 'harapan kosong', 'hampir gagal','jelek','mudah','pura','terbatas', 'tunggal', 'terlalu banyak', 'kekeliruan',
                     'kehilangan kontrol', 'kurang kompeten', 'risiko tinggi', 'kesalahan fatal','dijajah', 'terjebak dalam masalah', 'gagal total', 'rugi besar', 'bahaya tinggi',
                     'kemunduran pesat', 'stagnasi', 'tertinggal jauh', 'keterpurukan besar', 'tertekan', 'terbanting', 'masalah besar', 'tertinggal dalam persaingan',
                     'kerugian besar', 'kesulitan luar biasa', 'kehilangan kesempatan','mohon','ganti', 'terhenti', 'kurang efektif', 'berisiko tinggi', 'keterpurukan', 'bahaya besar',
                     'buntu jalan', 'terlalu buruk', 'tertinggal jauh', 'terhambat sekali', 'lelah panjang', 'putus harapan', 'kehilangan daya', 'tidak membuahkan hasil',
                     'keterhambatan besar', 'tidak sesuai dengan ekspektasi', 'meleset', 'terlalu lama', 'tersandung masalah','kurang','rugi','tertinggal','salah',
                     'kesalahan','kurang','keliru','tawuran','dituntut','turun','berkurang','kasar','dikekang','permasalahan','beban','korupsi','kritik']

# Fungsi untuk melakukan labeling berdasarkan kata kunci
def label_sentiment_based_on_keywords(tfidf_row):
    # Menyaring kata-kata yang memiliki nilai TF-IDF > threshold (misalnya 0.05)
    relevant_words = tfidf_row[tfidf_row > 0.05].index.tolist()

    # Cek apakah ada kata kunci positif atau negatif
    positive_matches = [word for word in relevant_words if word in positive_keywords]
    negative_matches = [word for word in relevant_words if word in negative_keywords]

    # Tentukan label berdasarkan kata kunci yang ditemukan
    if positive_matches:
        return 'Positif'
    elif negative_matches:
        return 'Negatif'
    else:
        return 'Netral'

# Data teks yang sudah melalui lemmatization
# Misalnya df['LemmatizedText'] berisi data yang sudah dilemmatize
# Pastikan Anda sudah memiliki dataframe 'df' dengan kolom 'LemmatizedText'

# Terapkan TF-IDF pada data lemmatized
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['Lemmatization_text'])

# Mengubah hasil TF-IDF menjadi dataframe agar lebih mudah diproses
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Terapkan fungsi labeling pada hasil TF-IDF
df['Label'] = tfidf_df.apply(label_sentiment_based_on_keywords, axis=1)

# Menampilkan hasil hanya kolom Label
display(df[['Lemmatization_text', 'Label']].head(5))

# Menghitung jumlah tweet dengan sentimen positif, negatif, dan netral
sentiment_counts = df['Label'].value_counts()

# Menampilkan hasil
print(sentiment_counts)

"""#**SVM**"""

# Import libraries
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already present
nltk.download('stopwords')

# Get Indonesian stop words
stop_words = set(stopwords.words('indonesian'))

# Vektorisasi teks menggunakan TF-IDF
vectorizer = TfidfVectorizer(stop_words=list(stop_words), max_features=5000)  # Menggunakan 5000 fitur terbaik
# Convert list of tokens back to string and fit transform
X = vectorizer.fit_transform(df['Stopword'].apply(lambda x: ' '.join(x)))

# Label sentimen: Positif, Negatif, atau Netral
y = df['Label'].map({'Positif': 1, 'Negatif': 0, 'Netral': 2})  # Menyusun label menjadi numerik

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Melatih model SVM
svm_model = SVC(kernel='linear', decision_function_shape='ovr')  # Kernel linear untuk klasifikasi multi-kelas
svm_model.fit(X_train, y_train)

# Memprediksi hasil dengan data uji
y_pred = svm_model.predict(X_test)

# Evaluasi model
print("Accuracy:", accuracy_score(y_test, y_pred))  # Menghitung akurasi
print(classification_report(y_test, y_pred))  # Menampilkan laporan klasifikasi (precision, recall, f1-score)

"""Interpretasi:
Hasil evaluasi model menunjukkan bahwa akurasi model adalah 0.70, yang berarti bahwa model berhasil mengklasifikasikan sekitar 70% dari data dengan benar. Rinciannya untuk masing-masing kelas adalah sebagai berikut:

- **Label 0 (Negatif)**: Model memiliki precision yang sangat tinggi (1.00), artinya ketika model memprediksi sebagai negatif, hampir seluruhnya benar. Namun, recall-nya rendah (0.39), yang menunjukkan bahwa model hanya berhasil mendeteksi 39% dari tweet yang sebenarnya negatif. Ini mengindikasikan bahwa banyak tweet negatif yang terlewat.
  
- **Label 1 (Netral)**: Precision untuk kelas netral adalah 0.33, artinya model tidak cukup akurat saat memprediksi tweet sebagai netral. Namun, recall-nya lebih baik (0.45), yang menunjukkan bahwa model bisa mendeteksi 45% dari tweet yang sebenarnya netral, meskipun ada banyak tweet netral yang salah diklasifikasikan.

- **Label 2 (Positif)**: Precision model untuk kelas positif adalah 0.67, artinya model cukup akurat dalam memprediksi tweet positif. Recall-nya juga baik (0.81), yang berarti model berhasil mendeteksi 81% dari tweet yang sebenarnya positif.

Secara keseluruhan, model memiliki kinerja yang lebih baik dalam mengidentifikasi tweet dengan sentimen positif, namun perlu diperbaiki untuk klasifikasi tweet dengan sentimen negatif dan netral. F1-score tertinggi tercatat pada kelas positif, sementara kelas negatif dan netral memerlukan peningkatan baik dari segi precision maupun recall.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Memprediksi hasil dengan data uji
y_pred = svm_model.predict(X_test)

# Menghitung matriks kebingungan
cm = confusion_matrix(y_test, y_pred)

# Menampilkan matriks kebingungan dengan visualisasi heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Positif', 'Negatif', 'Netral'], yticklabels=['Positif', 'Negatif', 'Netral'])
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Matriks Kebingungan untuk Model SVM')
plt.show()

"""# STOP HERE BESTIE!!!"""

# Filter tweet dengan sentimen Netral
neutral_tweets = df[df['Label'] == 'Netral']

# Menampilkan kolom Stopword dan Label hanya untuk sentimen Netral
display(neutral_tweets[['Stopword', 'Label']].head(155))

from nltk.tokenize import word_tokenize
import nltk

# Pastikan untuk mendownload resources yang diperlukan jika belum
nltk.download('punkt')

# Fungsi untuk tokenisasi
def tokenize_text(text):
    return word_tokenize(text)

# Terapkan tokenisasi pada kolom Normalisasi
df['Tokenisasi'] = df['Normalisasi'].apply(lambda x: tokenize_text(x))

# Menampilkan hasil
display(df[['full_text', 'Normalisasi', 'Tokenisasi']].head(5))







import nltk

# Download the 'punkt_tab' resource
nltk.download('punkt_tab')

# Fungsi untuk menghapus kata-kata tertentu
def hapus_kata(str_text):
    words = str_text.split()
    filtered_text = [word for word in words if word.lower() not in kata_untuk_dihapus]
    return " ".join(filtered_text)

# Fungsi untuk tokenisasi teks
def tokenizing_text(text):
    return word_tokenize(text)

# Gabungkan normalisasi, penghapusan kata, dan tokenisasi
df['Normalisasi_HapusKata'] = df['Normalisasi'].apply(lambda x: hapus_kata(normalisasi(x)))
df['Tokenizing'] = df['Normalisasi_HapusKata'].apply(lambda x: tokenizing_text(x))

# Menampilkan hanya kolom 'full_text', 'Normalisasi_HapusKata', dan 'Tokenizing'
display(df[['full_text', 'Normalisasi_HapusKata', 'Tokenizing']].head(10))

from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Gabungkan semua token di kolom 'Tokenizing' menjadi satu string
all_text = " ".join([" ".join(tokens) for tokens in df['Tokenizing']])

# Visualisasi word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords='indonesian').generate(all_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title("Word Cloud setelah Preprocessing")
plt.axis("off")
plt.show()

print(df.columns)

"""### DATA FINAL PREPROCESSING"""

display(df.head())

"""### PEMBOBOTAN COBA BEBERAPA KATA"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Misalkan data tersimpan dalam file CSV
documents = df['Normalisasi'].astype(str).tolist()  # Konversi ke list string

# Inisialisasi TfidfVectorizer
vectorizer = TfidfVectorizer()

# Transformasi dokumen ke matriks TF-IDF
tfidf_matrix = vectorizer.fit_transform(documents)

# Konversi hasilnya ke dalam DataFrame Pandas
feature_names = vectorizer.get_feature_names_out()
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

# Filter kolom untuk kata-kata tertentu
target_words = ['kurikulum', 'merdeka', 'belajar']
df_filtered = df_tfidf[target_words]

# Menampilkan sebagian data (misal: 5 baris pertama & terakhir)
display(df_filtered)

"""### TF IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Membaca data dari file (contoh format Excel)
documents = df['Normalisasi'].astype(str).tolist()  # Kolom teks diubah ke list string

# Inisialisasi TfidfVectorizer
vectorizer = TfidfVectorizer(
    max_features=5000,         # Jumlah maksimal fitur (kata) yang akan digunakan
    stop_words='english',      # Menghapus stopwords bahasa Inggris (ubah jika perlu)
    ngram_range=(1, 2),        # Menggunakan unigram dan bigram
    max_df=0.85,               # Mengabaikan kata yang muncul di lebih dari 85% dokumen
    min_df=5,                  # Mengabaikan kata yang muncul kurang dari 5 kali
    norm='l2',                 # Normalisasi L2 untuk pembobotan
    use_idf=True,              # Aktifkan komponen IDF
    smooth_idf=True            # Gunakan smoothing pada IDF
)
# Transformasi dokumen ke matriks TF-IDF
tfidf_matrix = vectorizer.fit_transform(documents)

# Melihat fitur (kata/term) yang dihasilkan
feature_names = vectorizer.get_feature_names_out()
# Konversi hasil matriks TF-IDF ke DataFrame untuk visualisasi
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

# Menampilkan sebagian hasil
print("5 Baris Pertama Matriks TF-IDF:")
print(df_tfidf.head())

print("\n5 Baris Terakhir Matriks TF-IDF:")
print(df_tfidf.tail())

# Jika ingin menyimpan hasil ke file CSV
df_tfidf.to_csv("hasil_tfidf.csv", index=False)

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from google.colab import files

# Upload the file to Colab's environment if it's not there already
uploaded = files.upload()

# Assuming the file was uploaded, get the name
file_path = list(uploaded.keys())[0]  # Get the name of the uploaded file
# Membaca data dari file (contoh format Excel)
# file_path = "Normalisasi_HapusKata.xlsx"  # Ganti dengan file yang sesuai
df = pd.read_excel(file_path)  # Membaca file Excel
documents = df['Normalisasi_HapusKata'].astype(str).tolist()  # Kolom teks diubah ke list string

# Inisialisasi TfidfVectorizer
vectorizer = TfidfVectorizer(
    max_features=5000,         # Jumlah maksimal fitur (kata) yang akan digunakan
    stop_words='english',      # Menghapus stopwords bahasa Inggris (ubah jika perlu)
    ngram_range=(1, 2),        # Menggunakan unigram dan bigram
    max_df=0.85,               # Mengabaikan kata yang muncul di lebih dari 85% dokumen
    min_df=5,                  # Mengabaikan kata yang muncul kurang dari 5 kali
    norm='l2',                 # Normalisasi L2 untuk pembobotan
    use_idf=True,              # Aktifkan komponen IDF
    smooth_idf=True            # Gunakan smoothing pada IDF
)
# Transformasi dokumen ke matriks TF-IDF
tfidf_matrix = vectorizer.fit_transform(documents)

# Melihat fitur (kata/term) yang dihasilkan
feature_names = vectorizer.get_feature_names_out()
# Konversi hasil matriks TF-IDF ke DataFrame untuk visualisasi
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

# Menampilkan sebagian hasil
print("5 Baris Pertama Matriks TF-IDF:")
print(df_tfidf.head())

print("\n5 Baris Terakhir Matriks TF-IDF:")
print(df_tfidf.tail())

# Jika ingin menyimpan hasil ke file CSV
df_tfidf.to_csv("hasil_tfidf.csv", index=False)

from textblob import TextBlob

def label_sentiment(text):
    # Analisis sentimen dengan TextBlob
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment > 0:
        return 'Positif'
    elif sentiment < 0:
        return 'Negatif'
    else:
        return 'Netral'

# Terapkan fungsi analisis sentimen untuk menambahkan label
df['Label'] = df['Normalisasi'].apply(label_sentiment)

# Menampilkan data yang telah dilabelkan
display(df)

sentimen = df['Label'].value_counts()
print(sentimen)

from google.colab import files

# Simpan data yang dilabelkan ke dalam file CSV
df.to_csv("data_dilabelkan.csv", index=False)

# Mengunduh file yang telah disimpan
files.download("data_dilabelkan.csv")

# Membagi data menjadi data training dan testing
X_train, X_test, y_train, y_test = train_test_split(
    tfidf_matrix, labels, test_size=0.2, random_state=42
)
# Inisialisasi model SVM
svm_model = SVC(kernel='linear', random_state=42)

# Latih model menggunakan data training
svm_model.fit(X_train, y_train)

!pip install textblob
import pandas as pd
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer

# Function to label the sentiment of a text
def label_sentiment(text):
    # Sentiment analysis with TextBlob
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment > 0:
        return 'Positif'
    elif sentiment < 0:
        return 'Negatif'
    else:
        return 'Netral'

# Load data (replace with your data loading code)
df = pd.read_excel("Normalisasi_HapusKata.xlsx")

# Apply sentiment analysis to add labels
df['Label'] = df['Normalisasi_HapusKata'].apply(label_sentiment)

# TF-IDF Feature Extraction
documents = df['Normalisasi_HapusKata'].astype(str).tolist()
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2), max_df=0.85, min_df=5, norm='l2', use_idf=True, smooth_idf=True)
tfidf_matrix = vectorizer.fit_transform(documents)

# Extract labels
labels = df['Label'].tolist()

# Check for the number of unique classes
unique_classes = set(labels)
if len(unique_classes) <= 1:
    raise ValueError("Your target variable has only one class. You need at least two classes for SVM training.")

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)

# Initialize SVM model
svm_model = SVC(kernel='linear', random_state=42)

# Train the model using the training data
svm_model.fit(X_train, y_train)

# Predictions on the testing data
y_pred = svm_model.predict(X_test)

# Evaluate the model's performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred))

from textblob import TextBlob

def label_sentiment(text):
    # Analisis sentimen dengan TextBlob
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment > 0:
        return 'Positif'
    elif sentiment < 0:
        return 'Negatif'
    else:
        return 'Netral'

# Terapkan fungsi analisis sentimen untuk menambahkan label
df['Label'] = df['df_tfidf'].apply(label_sentiment)

# Menampilkan data yang telah dilabelkan
print(df.head())
print(df)

"""## BATAS!"""

import pandas as pd
from google.colab import files

# Mengambil 5 baris pertama dari DataFrame df
head_df = df['Normalisasi_HapusKata']

# Menyimpan ke dalam file Excel
head_df.to_excel('df_head.xlsx', index=False)  # Simpan di direktori lokal

# Download file .xlsx
files.download('df_head.xlsx')

"""### Stemming, lemmatization, dan tokenisasi"""

!pip install nltk Sastrawi

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Download stopwords dan wordnet dari nltk
nltk.download('stopwords')
nltk.download('wordnet')

# Inisialisasi stop words, stemmer, dan lemmatizer
stop_words = set(stopwords.words('indonesian'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Lihat kolom-kolom di DataFrame
print("Kolom-kolom yang tersedia:", df.columns)

# Fungsi untuk membersihkan teks
def preprocess_text(text):
    # 1. Konversi ke huruf kecil
    text = str(text).lower()
    # 2. Hapus karakter khusus
    text = re.sub(r'[^a-z\s]', '', text)
    # 3. Tokenisasi
    words = text.split()
    # 4. Hapus stop words
    words = [word for word in words if word not in stop_words]
    # 5. Stemming atau lemmatization (gunakan salah satu)
    words = [stemmer.stem(word) for word in words]  # Stemming
    # words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization

    # Gabungkan kembali kata-kata yang sudah diproses
    return ' '.join(words)

# Terapkan fungsi pembersihan ke kolom teks
# Pastikan untuk mengganti 'nama_kolom_teks' dengan nama kolom teks yang sesuai di file Anda
df['cleaned_text'] = df['full_text'].apply(preprocess_text)

print("Data berhasil diproses dan disimpan sebagai cleaned_data.xlsx.")

from IPython.display import display

# Menampilkan data
display(df[['created_at', 'full_text', 'cleaned_text']].head())

import pandas as pd

# Membaca data dari file Excel yang ada
df_existing = pd.read_excel("Data Mentah Hasil Scrapping.xlsx")

# Menambahkan kolom 'cleaned_text' ke DataFrame yang ada
df_existing['cleaned_text'] = df['cleaned_text']

# Menyimpan DataFrame yang telah diperbarui ke file Excel baru
df_existing.to_excel("Data Mentah Hasil Scrapping.xlsx", index=False)

print("Kolom 'cleaned_text' berhasil ditambahkan ke dalam 'Data Mentah Hasil Scrapping.xlsx'.")

files.download("Data Mentah Hasil Scrapping.xlsx")